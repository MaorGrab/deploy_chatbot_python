## ðŸ¤– RAG-Enhanced Chatbot using Dash, FastAPI, and OpenAI

This repository contains a **Retrieval-Augmented Generation (RAG)** based chatbot application that combines **Dash** (for UI), **FastAPI** (for backend), **Uvicorn** (ASGI server), **OpenAI GPT** models (for language understanding), and **LlamaIndex** (for document indexing and retrieval).

The chatbot is capable of answering user questions based on your own local data via a user-friendly web dashboard.

![](sample_image.png)

---

## ðŸš€ Features

- âœ… **Dash-based UI** for interaction
- âœ… **FastAPI backend** with clean routing
- âœ… **OpenAI GPT** integration for language generation
- âœ… **LlamaIndex** for context-aware RAG pipeline
- âœ… **Hash-based caching (Smart re-indexing)** Automatically updates index if training files to avoid redundant indexing
- âœ… **Hot-reload and modular launch system**
- âœ… **Separation of concerns** for scalability and maintenance

---

## ðŸ“ Project Structure

```

deploy_chatbot_python/               # Root of your Git repository
â”œâ”€â”€ data/                            # Training data
â”‚   â””â”€â”€ training/
â”‚       â””â”€â”€ chatbot_data.txt         # Sample data for demonstration
|
â”œâ”€â”€ deploy_chatbot_python/           # Python package
â”‚   â”œâ”€â”€ __main__.py                  # Entry point - main script
â”‚   â”œâ”€â”€ launcher.py                  # Launches full stack (API, frontend, etc.)
â”‚   â”‚
â”‚   â”œâ”€â”€ backend/                     # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ run.py                   # Server runner
â”‚   â”‚   â””â”€â”€ server.py                # API logic
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                      # Configuration
â”‚   â”‚   â”œâ”€â”€ config.yaml              # Core model settings
â”‚   â”‚   â””â”€â”€ constants.py             # Shared constants
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                        # Core logic for indexing and querying
â”‚   â”‚   â”œâ”€â”€ index_manager.py         # Hash-based index validation and management
â”‚   â”‚   â”œâ”€â”€ llama_indexer.py         # LlamaIndex setup and pipeline
â”‚   â”‚   â””â”€â”€ openai_params.py         # OpenAI configuration structure
â”‚   â”‚
â”‚   â”œâ”€â”€ frontend/                    # Dashboard
â”‚   â”‚   â”œâ”€â”€ callbacks.py             # Dashboard callbacks
â”‚   â”‚   â”œâ”€â”€ layout.py                # UI layout
â”‚   â”‚   â””â”€â”€ run.py                   # Launch dashboard
â”‚   â”‚
â”‚   â”œâ”€â”€ logging/                     # Centralized logging
â”‚   â”‚   â”œâ”€â”€ logger.py                # Logger setup
â”‚   â”‚   â””â”€â”€ logger_instance.py       # Shared logger instance
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # Utilities
â”‚       â””â”€â”€ load_env.py             # Load environment variables (i.e. API key)
â”‚
â”œâ”€â”€ tests/                           # Tests (outside the package)
â”‚   â”œâ”€â”€ test_backend.py              # FastAPI tests
â”‚   â””â”€â”€ test_core_query.py           # Integration test of query engine
â”‚
â””â”€â”€ docs/                            # Documentation
    â”œâ”€â”€ README.md                    # Project-specific documentation
    â””â”€â”€ sample_image.png             
 
```

Other project files:
- `pyproject.toml`: Build system management.
- `requirements.txt`: dependency management.
- `pytest.ini`: Test runner config.
- `LICENSE`

---
## ðŸ§  How It Works

* **User enters a question** in the Dash interface
* The message is sent to the **FastAPI backend**
* **LlamaIndex** fetches relevant context from `/data` directory
* The context + question are passed to **OpenAIâ€™s API**
* The generated response is returned and displayed in the UI

---

## ðŸš€ Getting Started

### 1. Installation

We recommend using a virtual environment:

```bash
python -m venv venv
source venv/bin/activate      # On Windows: venv\Scripts\activate
pip install -e .
```

This installs the package in editable mode so you can modify the source code during development.


### 2. Set OpenAI API key
1. Create a `.env` file (or remove the `.example` suffix from the `.env.example` file)
2. Set your OpenAI API key, within the `.env` file like so:
```
OPENAI_API_KEY=sk-proj-...
```

### 3. Run the package
In the CLI run:
```bash
python -m deploy_chatbot_python
```

### 4. Open the browser
See the logs for the browser URL to open.
(Default should be: `http://127.0.0.1:8050/`)

You should now see the chatbot interface and use it!

---

## ðŸ”§ Configurations

The `config/config.yaml` file contains runtime settings used to configure the behavior of the chatbot, especially its interaction with OpenAI's models. This file enables easy tuning of key parameters without modifying the codebase.

#### Example Structure (Default):

```yaml
openai:
  model: gpt-4o-mini
  embedding_model: text-embedding-3-small
  temperature: 0.1
```

#### Description of Fields:

* `model`: Specifies the OpenAI language model used for generating chatbot responses (e.g., `gpt-4o-mini`, `gpt-4`, etc.).
* `embedding_model`: Defines the model used to generate vector embeddings for text inputs, typically used in retrieval and indexing (e.g., `text-embedding-3-small`).
* `temperature`: Controls the creativity of the model's output. Lower values (e.g. `0.1`) make the responses more focused and deterministic; higher values make them more varied and creative.

> ðŸ’¡ Modify this YAML to switch models, adjust response style, or integrate newer API versionsâ€”without touching the Python code.

---

## ðŸ“„ License

This project is licensed under the terms of the MIT license.

Hereâ€™s a section you can add to your `README.md` to explain the `config/config.yaml` file:

---
