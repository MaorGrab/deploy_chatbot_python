## ğŸ¤– RAG-Enhanced Chatbot using Dash, FastAPI, and OpenAI

This repository contains a **Retrieval-Augmented Generation (RAG)** based chatbot application that combines **Dash** (for UI), **FastAPI** (for backend), **Uvicorn** (ASGI server), **OpenAI GPT** models (for language understanding), and **LlamaIndex** (for document indexing and retrieval).

The chatbot is capable of answering user questions based on your own local data via a user-friendly web dashboard.

![](sample_image.png)

---

## ğŸš€ Features

- âœ… **Dash-based UI** for interaction
- âœ… **FastAPI backend** with clean routing
- âœ… **OpenAI GPT** integration for language generation
- âœ… **LlamaIndex** for context-aware RAG pipeline
- âœ… **Hash-based caching (Smart re-indexing)** Automatically updates index if training files to avoid redundant indexing
- âœ… **Hot-reload and modular launch system**
- âœ… **Separation of concerns** for scalability and maintenance

---

## ğŸ“ Project Structure

```

deploy_chatbot_python/
â”œâ”€â”€ __main__.py                # Entry point - main script
â”œâ”€â”€ launcher.py                # Launches full stack (API, frontend, etc.)
â”‚
â”œâ”€â”€ backend/                   # FastAPI backend
â”‚   â”œâ”€â”€ run.py                 # Server runner
â”‚   â””â”€â”€ server.py              # API logic
â”‚
â”œâ”€â”€ config/                    # Configuration
â”‚   â”œâ”€â”€ config.yaml            # Core model settings
â”‚   â””â”€â”€ constants.py           # Shared constants
â”‚
â”œâ”€â”€ core/                      # Core logic for indexing and querying
â”‚   â”œâ”€â”€ index_manager.py       # Hash-based index validation and management
â”‚   â”œâ”€â”€ llama_indexer.py       # LlamaIndex setup and pipeline
â”‚   â””â”€â”€ openai_params.py       # OpenAI configuration structure
â”‚
â”œâ”€â”€ frontend/                  # Dashboard
â”‚   â”œâ”€â”€ callbacks.py           # Dashboard callbacks
â”‚   â”œâ”€â”€ layout.py              # UI layout
â”‚   â””â”€â”€ run.py                 # Launch dashboard
â”‚
â”œâ”€â”€ logging/                   # Centralized logging
â”‚   â”œâ”€â”€ logger.py              # Logger setup
â”‚   â””â”€â”€ logger_instance.py     # Shared logger instance
â”‚
â”œâ”€â”€ utils/                     # Utilities
â”‚   â””â”€â”€ load_env.py            # Load environment variables (i.e. API key)

â”œâ”€â”€ data/
â”‚   â””â”€â”€ training/              # Input training data (e.g., txt)
â”‚        â””â”€â”€ chatbot_data.txt  # Sample data for demostration. One should place relevant data here [!]
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_backend.py        # FastAPI tests
    â””â”€â”€ test_core_query.py     # Integration test of query engine

```

Other project files:
- `pyproject.toml`: Build system management.
- `requirements.txt`: dependency management.
- `pytest.ini`: Test runner config.
- `README.md`, `LICENSE`

---
## ğŸ§  How It Works

* **User enters a question** in the Dash interface
* The message is sent to the **FastAPI backend**
* **LlamaIndex** fetches relevant context from `/data` directory
* The context + question are passed to **OpenAIâ€™s API**
* The generated response is returned and displayed in the UI

---

## ğŸš€ Getting Started

### 1. Installation

We recommend using a virtual environment:

```bash
python -m venv venv
source venv/bin/activate      # On Windows: venv\Scripts\activate
pip install -e .
```

This installs the package in editable mode so you can modify the source code during development.


### 2. Set OpenAI API key
1. Create a `.env` file (or remove the `.example` suffix from the `.env.example` file)
2. Set your OpenAI API key, within the `.env` file like so:
```
OPENAI_API_KEY=sk-proj-...
```

### 3. Run the package
In the CLI run:
```bash
python -m deploy_chatbot_python
```

### 4. Open the browser
See the logs for the browser URL to open.
(Default should be: `http://127.0.0.1:8050/`)

You should now see the chatbot interface and use it!

---

## ğŸ“„ License

This project is licensed under the terms of the MIT license.
